from flask import Flask, request, jsonify
import pandas as pd
from pymysql import converters
import pymysql
from mysql_config import *
from datetime import datetime, timedelta
import statistics
import logging
from logging.handlers import RotatingFileHandler

import traceback
from time import strftime


app = Flask(__name__)

def connectToDB():

    conv=converters.conversions.copy()
    conv[246]=float    # convert decimals to floats
    conn = pymysql.connect(
        host=MYSQL_DATABASE_HOST,
        user=MYSQL_DATABASE_USER,
        password=MYSQL_DATABASE_PASSWORD,
        db=MYSQL_DATABASE_DB,
        cursorclass=pymysql.cursors.DictCursor,
        conv=conv
    )
    curr = conn.cursor()

    return conn, curr

@app.route('/update_monke_count', methods=["POST"])
def update_monke_count():
    conn, curr = connectToDB()
   
    player_count = request.values.get('player_count')
    if player_count is None:
        return 'No player count', 400

    try:
        data = {
            "player_count": int(request.values.get("player_count", 0)),
            "room_name": request.values.get("room_name", "none"),
            "game_version": request.values.get("game_version", "none"),
            "game_name": request.values.get("game_name", "none"),
        }
    except:
        return "ur a failure"

    query = """
    INSERT INTO Monke
    (
        player_count,
        room_name,
        game_version,
        game_name
    )
    VALUES
    (
        %(player_count)s,
        %(room_name)s,
        %(game_version)s,
        %(game_name)s
    );
    """
    curr.execute(query, data)
    conn.commit()
    curr.close()
    return "Success"

@app.route('/how_many_monke_graph', methods=["GET"])
def how_many_monke_graph():
    hours = request.args.get('hours', 24)
    
    try:
        hours = float(hours)
    except:
        print("Failed to parse hours arg. Just using 24")
        hours = 24
    conn, curr = connectToDB()


    old_hours = []

    # this is the fractional hour up until now
    last_hour = datetime.utcnow().replace(microsecond=0, second=0, minute=0)

    # calculate the new number of hours from this time to the beginning
    start_time = datetime.utcnow() - timedelta(hours=hours)

    for i in range(0, int((last_hour - start_time).total_seconds() / 3600)):
        old_hours.append(last_hour - timedelta(hours=i+1))
        
    # also add the last fractional bit.
    old_hours.append(start_time)
    old_hours = list(set(old_hours))

    all_data = []
    for old_hour in old_hours:
        all_data.extend(get_data_from_hour(old_hour, conn, curr, True))
        
    # get the rest of the recent data without cache
    all_data.extend(get_data_from_hour(last_hour, conn, curr, False))

    conn.commit()
    curr.close()

    return jsonify(all_data)


def get_data_from_hour(hour_timestamp, conn, curr, should_add_to_cache: bool) -> list:

    start = hour_timestamp.isoformat()
    end = (hour_timestamp.replace(microsecond=0, second=0, minute=0) + timedelta(hours=1)).isoformat()

    # fstring not safe, but ok since generated by us
    query = f"""
    SELECT `timestamp`, `player_count`
    FROM `Cache`
    WHERE `timestamp` > "{start}"
    AND `timestamp` < "{end}"
    ORDER BY `timestamp` DESC;
    """
    curr.execute(query)
    output = [dict(row) for row in curr.fetchall()]

    # cache hit
    if len(output) != 0:
        return output

        
    # cache miss
    # fetch from big boy database
    query = f"""
    SELECT `timestamp`, `player_count`
    FROM `Monke`
    WHERE `timestamp` > '{start}'
    AND `timestamp` < '{end}'
    ORDER BY `timestamp` DESC;
    """
    curr.execute(query)
    output = [dict(row) for row in curr.fetchall()]

    if len(output) == 0:
        return output
    
    df = pd.DataFrame(output)
    df.columns = ['timestamp', 'player_count']
    df['player_count'] = pd.to_numeric(df['player_count'], errors='coerce')
    df = df.dropna()
    df = df.astype({'player_count': 'int32'})
    length = 500
    df['player_count'] = df['player_count'].rolling(int(length + 1)).median().shift(int(-length/2))
    for i in range (0, 2):
        df = df[df['player_count'] > df['player_count'].shift(-1) - 100]
        df = df[df['player_count'] < df['player_count'].shift(-1) + 100]
    output = df.to_dict('records')

    output = output[::max(1,int(len(output)/30))]

    if should_add_to_cache:
        add_to_cache(output, conn, curr)
        
    return output


@app.route('/how_many_monke', methods=["GET"])
def how_many_monke():
    conn, curr = connectToDB()

    query = """
    SELECT `player_count`
    FROM `Monke`
    WHERE `player_count` > 0 AND `player_count` < 10000
    ORDER BY `timestamp` DESC
    LIMIT 100;
    """
    curr.execute(query)
    most_recent_update = [dict(row)['player_count'] for row in curr.fetchall()]
    conn.close()
    median = statistics.median(most_recent_update)

    return jsonify(median)
    

def add_to_cache(data, conn, curr):
    query = """
    INSERT INTO `Cache`
    (
        `timestamp`,
        `player_count`
    )
    VALUES
    (
        %(timestamp)s,
        %(player_count)s
    );
    """
    for row in data:
        curr.execute(query, row)
    return "Success"



handler = RotatingFileHandler('app.log', maxBytes=100000000, backupCount=1500)
logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)
logger.addHandler(handler)

@app.after_request
def after_request(response):
    """ Logging after every request. """
    # This avoids the duplication of registry in the log,
    # since that 500 is already logged via @app.errorhandler.
    if response.status_code != 500:
        ts = strftime('[%Y-%b-%d %H:%M]')
        logger.error('%s %s %s %s %s %s',
                     ts,
                     request.remote_addr,
                     request.method,
                     request.scheme,
                     request.full_path,
                     response.status)
    return response


@app.errorhandler(Exception)
def exceptions(e):
    """ Logging after every Exception. """
    ts = strftime('[%Y-%b-%d %H:%M]')
    tb = traceback.format_exc()
    logger.error('%s %s %s %s %s 5xx INTERNAL SERVER ERROR\n%s',
                 ts,
                 request.remote_addr,
                 request.method,
                 request.scheme,
                 request.full_path,
                 tb)

    return "SERVER ERROR", 500
